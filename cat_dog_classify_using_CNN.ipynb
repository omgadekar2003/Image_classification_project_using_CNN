{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGjzS-FxB0rN",
        "outputId": "1df8ee42-e87d-4281-c5b2-08387725afbf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-13 07:42:06--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.212.207, 173.194.215.207, 173.194.216.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.212.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   114MB/s    in 0.6s    \n",
            "\n",
            "2024-10-13 07:42:07 (114 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "IP5CIT_UBtfL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kGZM02-p7jdM"
      },
      "outputs": [],
      "source": [
        "import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from keras .models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "YRXakktOD0jo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN Network:**"
      ],
      "metadata": {
        "id": "S0FJ-zJIEsSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = Sequential()"
      ],
      "metadata": {
        "id": "froC6p24EmUP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filters size, kernel size, size should be 64*64 pixels & in 3 color channel\n",
        "# activation function reLU:\n",
        "# then adding MaxPooling2D with pool_size(2,2)\n",
        "# Repeating this again to reduce image size pixels\n",
        "\n",
        "cnn.add(Conv2D((32),(3,3),input_shape=(64,64,3),activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn.add(Conv2D((16),(3,3),activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn.add(Flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX7wo0MXEmcQ",
        "outputId": "58735469-0c33-4e74-b9d7-474b04a7bd58"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANN Network**"
      ],
      "metadata": {
        "id": "UxWvbSpTHc9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using relu in activation function and at last we wnat binary output as 0 or 1:\n",
        "# so at last we're using sigmoid activation function:\n",
        "cnn.add(Dense(units=64,activation='relu'))\n",
        "cnn.add(Dense(units=32,activation='relu'))\n",
        "cnn.add(Dense(units=16,activation='relu'))\n",
        "cnn.add(Dense(units=8,activation='relu'))\n",
        "cnn.add(Dense(units=4,activation='relu'))\n",
        "cnn.add(Dense(units=2,activation='relu'))\n",
        "cnn.add(Dense(units=1,activation='sigmoid'))"
      ],
      "metadata": {
        "id": "-7GVvGylEmjt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile modle:\n",
        "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "i75i-duEEmq8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training testing code from official keras Documentation\n",
        "# documentation link: https://faroit.com/keras-docs/1.2.0/preprocessing/image/"
      ],
      "metadata": {
        "id": "bTYyhlS5Jiw0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        r'/tmp/cats_and_dogs_filtered',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        r'/tmp/cats_and_dogs_filtered',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "\n",
        "cnn.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=100,\n",
        "        epochs=15,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=50,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsfRP0w2Jism",
        "outputId": "41515559-fc2c-4c36-cb1a-8e841ade1c67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 images belonging to 2 classes.\n",
            "Found 3000 images belonging to 2 classes.\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m 94/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6734 - loss: 0.6417"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 248ms/step - accuracy: 0.6730 - loss: 0.6418 - val_accuracy: 0.6637 - val_loss: 0.6553\n",
            "Epoch 2/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 222ms/step - accuracy: 0.6621 - loss: 0.6449 - val_accuracy: 0.6714 - val_loss: 0.6335\n",
            "Epoch 3/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 233ms/step - accuracy: 0.6712 - loss: 0.6348 - val_accuracy: 0.6612 - val_loss: 0.6392\n",
            "Epoch 4/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 203ms/step - accuracy: 0.6761 - loss: 0.6335 - val_accuracy: 0.6750 - val_loss: 0.6359\n",
            "Epoch 5/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 230ms/step - accuracy: 0.6852 - loss: 0.6234 - val_accuracy: 0.6612 - val_loss: 0.6385\n",
            "Epoch 6/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 246ms/step - accuracy: 0.6722 - loss: 0.6326 - val_accuracy: 0.6793 - val_loss: 0.6269\n",
            "Epoch 7/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 211ms/step - accuracy: 0.6685 - loss: 0.6359 - val_accuracy: 0.6712 - val_loss: 0.6315\n",
            "Epoch 8/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 205ms/step - accuracy: 0.6731 - loss: 0.6319 - val_accuracy: 0.6657 - val_loss: 0.6344\n",
            "Epoch 9/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 232ms/step - accuracy: 0.6657 - loss: 0.6366 - val_accuracy: 0.6687 - val_loss: 0.6375\n",
            "Epoch 10/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 207ms/step - accuracy: 0.6639 - loss: 0.6370 - val_accuracy: 0.6736 - val_loss: 0.6268\n",
            "Epoch 11/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 233ms/step - accuracy: 0.6589 - loss: 0.6394 - val_accuracy: 0.6569 - val_loss: 0.6371\n",
            "Epoch 12/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 250ms/step - accuracy: 0.6613 - loss: 0.6372 - val_accuracy: 0.6514 - val_loss: 0.6389\n",
            "Epoch 13/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 230ms/step - accuracy: 0.6663 - loss: 0.6321 - val_accuracy: 0.6525 - val_loss: 0.6355\n",
            "Epoch 14/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 227ms/step - accuracy: 0.6628 - loss: 0.6315 - val_accuracy: 0.6607 - val_loss: 0.6300\n",
            "Epoch 15/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 220ms/step - accuracy: 0.6742 - loss: 0.6206 - val_accuracy: 0.6637 - val_loss: 0.6253\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ce664598340>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**as we getting loss is very big but reason is simple to train this much big data model can take so much time**"
      ],
      "metadata": {
        "id": "dHo8HKl7NfVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing start:\n",
        "#image load\n",
        "from keras.preprocessing import image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8tkS8SUsJinu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload here new image to check the prediction of separation of cat & dog\n",
        "# r for relative path"
      ],
      "metadata": {
        "id": "mdtNZgFuSSOo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = image.load_img(r\"/content/drive/MyDrive/prediction data/2008.jpg\", target_size=(64,64))"
      ],
      "metadata": {
        "id": "RjHp9Sj1Emx2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert it into array:\n",
        "img = image.img_to_array(img)"
      ],
      "metadata": {
        "id": "KsRNw99wQufH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How image look into an array:\n",
        "img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMdQq9EkQzGx",
        "outputId": "77b46125-25dd-4031-a00b-6ec9541adb83"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 47.,  43.,  31.],\n",
              "        [ 58.,  54.,  42.],\n",
              "        [ 62.,  60.,  45.],\n",
              "        ...,\n",
              "        [ 71.,  69.,  56.],\n",
              "        [ 68.,  66.,  53.],\n",
              "        [ 67.,  60.,  50.]],\n",
              "\n",
              "       [[ 53.,  49.,  37.],\n",
              "        [ 62.,  58.,  46.],\n",
              "        [ 58.,  55.,  46.],\n",
              "        ...,\n",
              "        [ 71.,  69.,  56.],\n",
              "        [ 65.,  63.,  50.],\n",
              "        [ 48.,  48.,  38.]],\n",
              "\n",
              "       [[ 57.,  54.,  39.],\n",
              "        [ 65.,  62.,  47.],\n",
              "        [ 56.,  52.,  40.],\n",
              "        ...,\n",
              "        [ 65.,  63.,  50.],\n",
              "        [ 63.,  61.,  48.],\n",
              "        [ 23.,  22.,  20.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 18.,  17.,  15.],\n",
              "        [ 40.,  39.,  37.],\n",
              "        [ 17.,  17.,  15.],\n",
              "        ...,\n",
              "        [171., 175., 161.],\n",
              "        [172., 174., 163.],\n",
              "        [153., 139., 128.]],\n",
              "\n",
              "       [[ 18.,  17.,  15.],\n",
              "        [ 20.,  19.,  17.],\n",
              "        [ 19.,  19.,  17.],\n",
              "        ...,\n",
              "        [163., 169., 155.],\n",
              "        [181., 171., 162.],\n",
              "        [ 30.,  31.,  26.]],\n",
              "\n",
              "       [[ 25.,  24.,  22.],\n",
              "        [ 27.,  26.,  24.],\n",
              "        [ 22.,  21.,  19.],\n",
              "        ...,\n",
              "        [167., 160., 150.],\n",
              "        [ 26.,  33.,  26.],\n",
              "        [160., 167., 160.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# again we've to flatten it for Dimension Reduction\n",
        "# we use numpy here:"
      ],
      "metadata": {
        "id": "R35NoMJwQ0d7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = np.expand_dims(img, axis=0)"
      ],
      "metadata": {
        "id": "i9xqr7wmQ0n1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now add this image into prediction model\n",
        "p = cnn.predict(img)\n",
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW7CPtx3Q0sz",
        "outputId": "acb5344f-8c8a-4eda-97e4-261589243196"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00215755]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**as output is between 0 and 1 so we have to add the condition between less than 0.5 or more than 0.5**"
      ],
      "metadata": {
        "id": "1ZrTzoGiTJbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if p[0][0] < 0.5:\n",
        "  print(\"Dog\")\n",
        "else:\n",
        "  print(\"Cat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaH1xNcGSvUJ",
        "outputId": "c0d40394-7d31-403f-cd61-c879d86b06a3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We get Correct output as image of DOG"
      ],
      "metadata": {
        "id": "B94sYear4Kvs"
      }
    }
  ]
}